{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer:\n",
    "    - An input layer (or visible layer) receives an input from a dataset.\n",
    "### Hidden Layer:\n",
    "    - Hidden layers are layers between the input and output layers.  They can only be accessed through the input layer thus, we do not directly interact with them.\n",
    "### Output Layer:\n",
    "    - Output layers are the final layers of a neural network and is typically a vector of predictions.\n",
    "### Neuron:\n",
    "    - A Neuron is the basic unit of a neural network accepting any number of inputs and weights\n",
    "![Neuron](https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/47_blog_image_2.png)\n",
    "### Weight:\n",
    "    - A weight is a modifier representing the strength of a connection between two layers.  Weights close to 0 mean changing the input will not afftect the output.  Negative weights mean an increasing input will decrease the output.\n",
    "### Activation Function:\n",
    "    - Activation functions introduce non-linearity into a neural network\n",
    "    - They are sort of like trigger hairs of a nematocyst or a Venus Fly Trap in that it will determine whether the neuron fires.\n",
    "![Activation Functions](https://hackernoon.com/hn-images/1*p_hyqAtyI8pbt2kEl6siOQ.png)\n",
    "### Node Map:\n",
    "    - Node maps are visual diagrams of the layout of a neural network.\n",
    "![Node Map](https://miro.medium.com/max/2636/1*3fA77_mLNiJTSgZFhYnU0Q.png)\n",
    "### Perceptron:\n",
    "    - A perceptron is the simplest Neural Network consisting of a single node or neuron with nothing else taking any number of inputs and spitting out an output.\n",
    "![Perceptron](https://www.allaboutcircuits.com/uploads/articles/how-to-train-a-basic-perceptron-neural-network_rk_aac_image1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "An input is received and multiplied by a weight.  A bias of 1 is introduced as another input to offset a case where all other inputs would be None (0).  The neuron will continue to fire if it fits a given activation function such as a Sigmoid function (between 0 and 1).  All of these steps then lead to an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form of ' y = sigmoid(sum(w1x1 + w2x2) + b) '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "# Start by defining activation functions (sigmoids)\n",
    "class Sig():\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "    def sigmoid(self):\n",
    "        return 1 / (1 + np.exp(-self.x))\n",
    "    \n",
    "    def sigmoid_derivative(self):\n",
    "        sx = Sig(self.x).sigmoid()\n",
    "        return sx * (1 - sx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [0, 0, 1, 1]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the inputs\n",
    "l1 = df['x1'].to_list()\n",
    "l2 = df['x2'].to_list()\n",
    "inputs = np.array([l1,l2])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputs\n",
    "outputs = np.array([df['y'].to_list()])\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49778348],\n",
       "       [-0.48271525],\n",
       "       [ 0.94195234],\n",
       "       [ 0.79151575]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random weights for each input\n",
    "weights = 2 * np.random.random((4,1)) - 1\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30880051],\n",
       "       [1.7334681 ]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get weighted sum of inputs and weights\n",
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57659245],\n",
       "       [0.84985549]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid activation\n",
    "activated_output = Sig(weighted_sum).sigmoid()\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42340755,  0.42340755,  0.42340755, -0.57659245],\n",
       "       [ 0.15014451,  0.15014451,  0.15014451, -0.84985549]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the error\n",
    "error = outputs - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10336801,  0.10336801,  0.10336801, -0.14076559],\n",
       "       [ 0.01915861,  0.01915861,  0.01915861, -0.10844253]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent/backpropagation\n",
    "adjustments = error * Sig(weighted_sum).sigmoid_derivative()\n",
    "adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49778348,  0.49778348,  0.49778348,  0.49778348],\n",
       "       [-0.37934724, -0.37934724, -0.37934724, -0.62348083],\n",
       "       [ 0.96111095,  0.96111095,  0.96111095,  0.83350982],\n",
       "       [ 0.91404237,  0.91404237,  0.91404237,  0.54230764]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights + np.dot(inputs.T, adjustments)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after training\n",
      "[[ 0.49778348  0.49778348  0.49778348  0.49778348]\n",
      " [ 1.6613736   1.6613736   1.6613736  -1.98503055]\n",
      " [ 1.78247776  1.78247776  1.78247776 -1.89876404]\n",
      " [ 3.77613002  3.77613002  3.77613002 -3.55151594]]\n",
      "Output after training\n",
      "[[0.99566829 0.99566829 0.99566829 0.00392484]\n",
      " [0.99616048 0.99616048 0.99616048 0.00427695]]\n"
     ]
    }
   ],
   "source": [
    "# Iterate many times to reduce error\n",
    "for i in range(10000):\n",
    "    # Get weighted sum of inputs and weights\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    # Sigmoid activation\n",
    "    activated_output = Sig(weighted_sum).sigmoid()\n",
    "\n",
    "    # Calculate the error\n",
    "    error = outputs - activated_output\n",
    "\n",
    "    # Gradient descent/backpropagation\n",
    "    adjustments = error * Sig(weighted_sum).sigmoid_derivative()\n",
    "\n",
    "    weights = weights + np.dot(inputs.T, adjustments)\n",
    "\n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.60000</td>\n",
       "      <td>0.62700</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.60000</td>\n",
       "      <td>0.35100</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.30000</td>\n",
       "      <td>0.67200</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.10000</td>\n",
       "      <td>0.16700</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.10000</td>\n",
       "      <td>2.28800</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin      BMI  \\\n",
       "0            6      148             72             35        0 33.60000   \n",
       "1            1       85             66             29        0 26.60000   \n",
       "2            8      183             64              0        0 23.30000   \n",
       "3            1       89             66             23       94 28.10000   \n",
       "4            0      137             40             35      168 43.10000   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                   0.62700   50        1  \n",
       "1                   0.35100   31        0  \n",
       "2                   0.67200   32        1  \n",
       "3                   0.16700   21        0  \n",
       "4                   2.28800   33        1  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c0d248e1d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize class imbalance\n",
    "diabetes['Outcome'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.35294</td>\n",
       "      <td>0.74372</td>\n",
       "      <td>0.59016</td>\n",
       "      <td>0.35354</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.50075</td>\n",
       "      <td>0.23442</td>\n",
       "      <td>0.48333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.42714</td>\n",
       "      <td>0.54098</td>\n",
       "      <td>0.29293</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.39642</td>\n",
       "      <td>0.11657</td>\n",
       "      <td>0.16667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47059</td>\n",
       "      <td>0.91960</td>\n",
       "      <td>0.52459</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.34724</td>\n",
       "      <td>0.25363</td>\n",
       "      <td>0.18333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.44724</td>\n",
       "      <td>0.54098</td>\n",
       "      <td>0.23232</td>\n",
       "      <td>0.11111</td>\n",
       "      <td>0.41878</td>\n",
       "      <td>0.03800</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.68844</td>\n",
       "      <td>0.32787</td>\n",
       "      <td>0.35354</td>\n",
       "      <td>0.19858</td>\n",
       "      <td>0.64232</td>\n",
       "      <td>0.94364</td>\n",
       "      <td>0.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.58824</td>\n",
       "      <td>0.50754</td>\n",
       "      <td>0.62295</td>\n",
       "      <td>0.48485</td>\n",
       "      <td>0.21277</td>\n",
       "      <td>0.49031</td>\n",
       "      <td>0.03971</td>\n",
       "      <td>0.70000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.11765</td>\n",
       "      <td>0.61307</td>\n",
       "      <td>0.57377</td>\n",
       "      <td>0.27273</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.54844</td>\n",
       "      <td>0.11187</td>\n",
       "      <td>0.10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.29412</td>\n",
       "      <td>0.60804</td>\n",
       "      <td>0.59016</td>\n",
       "      <td>0.23232</td>\n",
       "      <td>0.13239</td>\n",
       "      <td>0.39046</td>\n",
       "      <td>0.07131</td>\n",
       "      <td>0.15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.63317</td>\n",
       "      <td>0.49180</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.44858</td>\n",
       "      <td>0.11571</td>\n",
       "      <td>0.43333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.05882</td>\n",
       "      <td>0.46734</td>\n",
       "      <td>0.57377</td>\n",
       "      <td>0.31313</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.45306</td>\n",
       "      <td>0.10120</td>\n",
       "      <td>0.03333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin     BMI  \\\n",
       "0        0.35294  0.74372        0.59016        0.35354  0.00000 0.50075   \n",
       "1        0.05882  0.42714        0.54098        0.29293  0.00000 0.39642   \n",
       "2        0.47059  0.91960        0.52459        0.00000  0.00000 0.34724   \n",
       "3        0.05882  0.44724        0.54098        0.23232  0.11111 0.41878   \n",
       "4        0.00000  0.68844        0.32787        0.35354  0.19858 0.64232   \n",
       "..           ...      ...            ...            ...      ...     ...   \n",
       "763      0.58824  0.50754        0.62295        0.48485  0.21277 0.49031   \n",
       "764      0.11765  0.61307        0.57377        0.27273  0.00000 0.54844   \n",
       "765      0.29412  0.60804        0.59016        0.23232  0.13239 0.39046   \n",
       "766      0.05882  0.63317        0.49180        0.00000  0.00000 0.44858   \n",
       "767      0.05882  0.46734        0.57377        0.31313  0.00000 0.45306   \n",
       "\n",
       "     DiabetesPedigreeFunction     Age  \n",
       "0                     0.23442 0.48333  \n",
       "1                     0.11657 0.16667  \n",
       "2                     0.25363 0.18333  \n",
       "3                     0.03800 0.00000  \n",
       "4                     0.94364 0.20000  \n",
       "..                        ...     ...  \n",
       "763                   0.03971 0.70000  \n",
       "764                   0.11187 0.10000  \n",
       "765                   0.07131 0.15000  \n",
       "766                   0.11571 0.43333  \n",
       "767                   0.10120 0.03333  \n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "# Removing columns from the data\n",
    "X = diabetes.drop('Outcome', axis=1)\n",
    "\n",
    "# Get our target vector\n",
    "y = diabetes['Outcome']\n",
    "\n",
    "# Scale the data\n",
    "mmscaler = MinMaxScaler()\n",
    "\n",
    "# Fit the data\n",
    "X_scaled = pd.DataFrame(mmscaler.fit_transform(X), columns=feats)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        self.weights = 2 * np.random.random((X.shape[1], 1)) - 1\n",
    "        \n",
    "        self.inputs = X.values.tolist()\n",
    "        \n",
    "        self.outputs = y.values.tolist()\n",
    "\n",
    "        for i in range(self.niter):\n",
    "                     \n",
    "            # Weighted sum of inputs / weights\n",
    "            self.weighted_sum = np.dot(self.inputs, self.weights)\n",
    "\n",
    "            # Activate!\n",
    "            self.activated_outputs = self.__sigmoid(self.weighted_sum)\n",
    "\n",
    "            # Cac error\n",
    "            self.error = self.outputs - self.activated_outputs\n",
    "            self.adjustments = self.error * self.__sigmoid_derivative(self.weighted_sum)\n",
    "            \n",
    "            # Update the Weights\n",
    "            self.weights = self.weights + np.dot(np.array(self.inputs).T, self.adjustments)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weights)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) == 0.0, 1, -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pn = Perceptron(10)\n",
    "pn.fit(X_scaled, y)\n",
    "\n",
    "y_pred = pn.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Neural Networks (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
